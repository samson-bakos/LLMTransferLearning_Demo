{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "534e18fb-19ff-465f-bd57-6da8b08a6e5a",
   "metadata": {},
   "source": [
    "# Sentiment Analysis With LLMs\n",
    "### Pre-trained Model vs Fine-Tuned\n",
    "\n",
    "This notebook was specifically constructed to work with MacOS MPS as a GPU. It has code that allows it to run on CUDA or CPU, but I have not validated if it works or not.\n",
    "\n",
    "This implementation uses custom native PyTorch to contrast the methods. For a more streamlined way to do this with built in methods, see this documentation on Fine-Tuning with Hugging Face: [https://huggingface.co/docs/transformers/training](https://huggingface.co/docs/transformers/training) \n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0e5239a-8d72-4c38-b681-a0a264799021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch\n",
    "# !pip install transformers\n",
    "# !pip install datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e11ce0e1-9ea6-4a74-810f-10ba0e1b5994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import torch.nn.functional as F \n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification, AdamW\n",
    "from datasets import load_dataset, load_metric\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f96dd220-2f04-42b5-b132-640b23addec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "# set up CUDA/MPS \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55f220ae-d738-4098-a823-6ece0fe35148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset containing text descriptions of movies, and a sentiment label\n",
    "dataset = load_dataset(\"imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80abc7ce-5018-46d3-982b-e1075a80914d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    dataset[\"train\"][\"text\"], dataset[\"train\"][\"label\"], test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8063902-08b5-4492-81f6-37a51a8cf07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load Tokenizer and BERT for Classification \n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "498c08ab-91a2-454f-a787-a4b6b0470715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and preprocess the data for both pre-training and fine-tuning\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=256)\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4f77e70-7a9e-48e0-a336-f9ae30d24833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prep Dataset Object\n",
    "class IMDbDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d172a9d-a9a9-4d89-931f-263703835630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoded train and test dataset\n",
    "train_dataset = IMDbDataset(train_encodings, train_labels)\n",
    "test_dataset = IMDbDataset(test_encodings, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32adca8c-deab-4047-b8a4-77b2e9e0f750",
   "metadata": {},
   "source": [
    "# Pre-Trained "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d567326-be60-4c98-a661-2d03da0c3b67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 1 Complete\n"
     ]
    }
   ],
   "source": [
    "# Load a pretrained model\n",
    "pretrained_model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Freeze it (no fine tuning)\n",
    "for param in pretrained_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Add a classification head\n",
    "classification_head = torch.nn.Linear(pretrained_model.config.hidden_size, 2)\n",
    "\n",
    "# Create a custom model class\n",
    "class CustomModel(torch.nn.Module):\n",
    "    def __init__(self, base_model, classification_head):\n",
    "        super(CustomModel, self).__init__()\n",
    "        self.base_model = base_model\n",
    "        self.classification_head = classification_head\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None):  # Add token_type_ids\n",
    "        outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)  # Pass token_type_ids\n",
    "        logits = self.classification_head(outputs.pooler_output)  # Use pooler_output for BERT-based models\n",
    "        return logits\n",
    "\n",
    "# Combine the base model and classification head\n",
    "model_frozen_weights = CustomModel(pretrained_model, classification_head)\n",
    "\n",
    "# Train the pre-trained model\n",
    "optimizer_pretrain = AdamW(model_frozen_weights.parameters(), lr=5e-5, no_deprecation_warning=True)\n",
    "train_loader_pretrain = torch.utils.data.DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# Move the optimizer state to GPU\n",
    "optimizer_pretrain.state = {key: value.to(device) for key, value in optimizer_pretrain.state.items()}\n",
    "\n",
    "# Training loop for pre-training with frozen weights\n",
    "# This loop redefines the optimizer in each loop because I was running into a type error caused by weird mps handling. \n",
    "# Its a bit verbose as a result.\n",
    "num_epochs_pretrain = 1\n",
    "count_epoch = 1\n",
    "for epoch in range(num_epochs_pretrain):\n",
    "    model_frozen_weights.train()\n",
    "    for batch in train_loader_pretrain:\n",
    "        # Move the model to the selected device\n",
    "        model_frozen_weights.to(device)\n",
    "\n",
    "        # Create a new optimizer for each batch\n",
    "        optimizer_pretrain = AdamW(model_frozen_weights.parameters(), lr=5e-5, no_deprecation_warning=True)\n",
    "\n",
    "        optimizer_pretrain.zero_grad()\n",
    "\n",
    "        # Move input tensors to the selected device\n",
    "        inputs = {key: value.to(device) for key, value in batch.items()}\n",
    "\n",
    "        # Pass token_type_ids to the model if available\n",
    "        if \"token_type_ids\" in inputs:\n",
    "            outputs = model_frozen_weights(inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"], token_type_ids=inputs[\"token_type_ids\"])\n",
    "        else:\n",
    "            outputs = model_frozen_weights(inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"])\n",
    "\n",
    "        logits = outputs\n",
    "        loss = F.cross_entropy(logits, inputs[\"labels\"])\n",
    "        loss.backward()\n",
    "        optimizer_pretrain.step()\n",
    "    print(f\"Training Epoch {count_epoch} Complete\")\n",
    "    count_epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f8a20b46-9a10-4ae5-9a36-a225de829454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for Pre-trained Model with Frozen Weights:\n",
      "Accuracy: 0.55\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.16      0.27      2515\n",
      "           1       0.53      0.94      0.68      2485\n",
      "\n",
      "    accuracy                           0.55      5000\n",
      "   macro avg       0.64      0.55      0.47      5000\n",
      "weighted avg       0.64      0.55      0.47      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Move the model to MPS\n",
    "model_frozen_weights = model_frozen_weights.to(device)\n",
    "model_frozen_weights.eval()\n",
    "predictions_pretrain = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in torch.utils.data.DataLoader(test_dataset, batch_size=8):\n",
    "        # Move the model to the selected device\n",
    "        model_frozen_weights.to(device)\n",
    "\n",
    "        inputs = batch[\"input_ids\"]\n",
    "        inputs = inputs.to(device)  # Move inputs to MPS device\n",
    "\n",
    "        outputs = model_frozen_weights(input_ids=inputs)\n",
    "        predictions_pretrain.extend(torch.argmax(outputs, dim=1).tolist())\n",
    "\n",
    "# Calculate and print performance metrics for the pre-trained model \n",
    "accuracy_pretrain = accuracy_score(test_labels, predictions_pretrain)\n",
    "report_pretrain = classification_report(test_labels, predictions_pretrain)\n",
    "\n",
    "print(\"\\nResults for Pre-trained Model with Frozen Weights:\")\n",
    "print(f\"Accuracy: {accuracy_pretrain:.2f}\")\n",
    "print(\"Classification Report:\\n\", report_pretrain)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c361a87-2f8c-4602-a8a5-70b19602b05c",
   "metadata": {},
   "source": [
    "## Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3e2e741-66c6-49f5-8c6d-3c4ac2ffe7f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 1 Complete\n"
     ]
    }
   ],
   "source": [
    "# Fine-tuned Model\n",
    "finetuned_model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "optimizer_finetune = AdamW(finetuned_model.parameters(), lr=5e-5, no_deprecation_warning=True)\n",
    "\n",
    "# Move the model to the selected device\n",
    "finetuned_model.to(device)\n",
    "\n",
    "# Move the optimizer state to selected device\n",
    "optimizer_finetune.state = {key: value.to(device) for key, value in optimizer_finetune.state.items()}\n",
    "\n",
    "# Fine-tuning loop for the fine-tuned model\n",
    "num_epochs_finetune = 1\n",
    "count_epoch = 1\n",
    "\n",
    "for epoch in range(num_epochs_finetune):\n",
    "    finetuned_model.train()\n",
    "    \n",
    "    # Create a new optimizer for each epoch\n",
    "    optimizer_finetune = AdamW(finetuned_model.parameters(), lr=5e-5, no_deprecation_warning=True)\n",
    "\n",
    "    for batch in torch.utils.data.DataLoader(train_dataset, batch_size=8, shuffle=True):\n",
    "        optimizer_finetune.zero_grad()\n",
    "\n",
    "        # Move input tensors to the selected device\n",
    "        inputs = {key: value.to(device) for key, value in batch.items()}\n",
    "\n",
    "        outputs = finetuned_model(**inputs)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer_finetune.step()\n",
    "    print(f\"Training Epoch {count_epoch} Complete\")\n",
    "    count_epoch += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f47b83ac-8d27-4e62-becf-c8f9536128d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for Fine-tuned Model:\n",
      "Accuracy: 0.90\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.88      0.90      2515\n",
      "           1       0.88      0.92      0.90      2485\n",
      "\n",
      "    accuracy                           0.90      5000\n",
      "   macro avg       0.90      0.90      0.90      5000\n",
      "weighted avg       0.90      0.90      0.90      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Move the model to MPS\n",
    "finetuned_model = finetuned_model.to(device)\n",
    "finetuned_model.eval()\n",
    "predictions_finetune = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in torch.utils.data.DataLoader(test_dataset, batch_size=8):\n",
    "        # Move the model to the selected device\n",
    "        finetuned_model.to(device)\n",
    "\n",
    "        inputs = {key: value.to(device) for key, value in batch.items()}\n",
    "\n",
    "        outputs = finetuned_model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        predictions_finetune.extend(torch.argmax(logits, dim=1).tolist())\n",
    "\n",
    "# Calculate and print performance metrics for fine-tuned model\n",
    "accuracy_finetune = accuracy_score(test_labels, predictions_finetune)\n",
    "report_finetune = classification_report(test_labels, predictions_finetune)\n",
    "\n",
    "print(\"\\nResults for Fine-tuned Model:\")\n",
    "print(f\"Accuracy: {accuracy_finetune:.2f}\")\n",
    "print(\"Classification Report:\\n\", report_finetune)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66432047-54f0-461a-96ff-5e1967c653b0",
   "metadata": {},
   "source": [
    "## Results Summary\n",
    "\n",
    "Fine tuning is much more accurate than using a frozen pre-trained model (91% vs 55% in one epoch on a balanced dataset). \n",
    "\n",
    "Fine tuning is more computationally and memory intensive. We are training all of the weights in the model, not just the final layers. We are required to load and track gradients for these parameters in memory as well.\n",
    "\n",
    "My general guidance: fine-tuning is the way to go. If you're coming to BERT and other high level contextual models, you're looking for high accuracy on challenging data. If you want to minimize computation in a classification problem, there are simpler methods than going to contextual embeddings. \n",
    "\n",
    "BERT \"understands\" natural language in general, and fine tuning allows us to make slight focusing adjustments to this understanding to achieve our goals. It's extremely powerful and most modern NLP applications are built on transfer learning in some form (i.e. using BERT or OpenAI's API). No need to start from scratch when we stand on the shoulders of giants with extremely powerful public models available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24da13d1-529d-4945-b68d-330ca4f2b8de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
